# Model Configuration
DEEPSEEK_MODEL=deepseek-ai/deepseek-llm-7b-base
HUGGINGFACE_TOKEN=your_hf_token_here
WANDB_API_KEY=your_wandb_key_here

# Training Configuration
BATCH_SIZE=4
LEARNING_RATE=2e-4
GRADIENT_ACCUMULATION_STEPS=8
MAX_LENGTH=2048
NUM_EPOCHS=3

# LoRA Configuration
LORA_RANK=16
LORA_ALPHA=32
LORA_DROPOUT=0.1
TARGET_MODULES=q_proj,v_proj,k_proj,o_proj

# Data Configuration
DATA_PATH=./data/processed/
OUTPUT_DIR=./checkpoints/
CACHE_DIR=./cache/

# Evaluation Configuration
EVAL_STEPS=500
SAVE_STEPS=1000
LOGGING_STEPS=50

# Deployment Configuration
API_HOST=0.0.0.0
API_PORT=8000
MODEL_CACHE_DIR=./model_cache/