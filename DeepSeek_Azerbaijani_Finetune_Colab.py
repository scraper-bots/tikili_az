# %% [markdown]
# # ğŸ‡¦ğŸ‡¿ DeepSeek Fine-tuning for Azerbaijani Language
#
# This notebook fine-tunes a language model specifically for Azerbaijani.
# Run each cell in order in Google Colab.

# %% [markdown]
# ## ğŸ“¦ Step 1: Install Dependencies

# %%
# Install required packages
!pip install transformers torch datasets accelerate peft bitsandbytes -q

# %%
# Import libraries
import torch
import json
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from datasets import Dataset
import os

print("âœ… All packages imported successfully!")
print(f"ğŸ”¥ CUDA available: {torch.cuda.is_available()}")
print(f"ğŸ“± Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}")

# %% [markdown]
# ## ğŸ“š Step 2: Prepare Azerbaijani Training Data

# %%
# Create Azerbaijani training dataset
azerbaijani_data = [
    {
        "instruction": "AzÉ™rbaycan haqqÄ±nda maraqlÄ± fakt sÃ¶ylÉ™",
        "output": "AzÉ™rbaycan dÃ¼nyada É™n Ã§ox vulkan palÃ§Ä±ÄŸÄ±na malik Ã¶lkÉ™dir. Burada 400-dÉ™n Ã§ox vulkan palÃ§Ä±ÄŸÄ± var."
    },
    {
        "instruction": "BakÄ±nÄ±n tarixi haqqÄ±nda mÉ™lumat ver",
        "output": "BakÄ± ÅŸÉ™hÉ™ri IX É™srdÉ™ yaranmÄ±ÅŸdÄ±r. O, XÉ™zÉ™r dÉ™nizi sahilindÉ™ yerlÉ™ÅŸir vÉ™ zÉ™ngin neft ehtiyatlarÄ± ilÉ™ mÉ™ÅŸhurdur."
    },
    {
        "instruction": "AzÉ™rbaycan mÉ™tbÉ™xindÉ™n yemÉ™klÉ™ri say",
        "output": "AzÉ™rbaycan mÉ™tbÉ™xindÉ™ plov, dolma, kebab, qutab, dÃ¼ÅŸbÉ™rÉ™, pakhlava kimi dadlÄ± yemÉ™klÉ™r var."
    },
    {
        "instruction": "AzÉ™rbaycanda neÃ§É™ iqlim qurÅŸaÄŸÄ± var",
        "output": "AzÉ™rbaycanda 9 iqlim qurÅŸaÄŸÄ± var: subtropik, mÃ¼layim kontinental, daÄŸ iqlimi vÉ™ digÉ™rlÉ™ri."
    },
    {
        "instruction": "Novruz bayramÄ± haqqÄ±nda danÄ±ÅŸ",
        "output": "Novruz baharÄ±n gÉ™liÅŸini qeyd edÉ™n qÉ™dim bayramdÄ±r. Bu bayram 21 mart tarixindÉ™ keÃ§irilir vÉ™ UNESCO tÉ™rÉ™findÉ™n qorunur."
    },
    {
        "instruction": "AzÉ™rbaycanÄ±n mÉ™ÅŸhur ÅŸairlÉ™rini say",
        "output": "AzÉ™rbaycanÄ±n mÉ™ÅŸhur ÅŸairlÉ™ri arasÄ±nda Nizami GÉ™ncÉ™vi, FÃ¼zuli, NÉ™simi, Sabir var."
    },
    {
        "instruction": "XÉ™zÉ™r dÉ™nizinin É™hÉ™miyyÉ™tini izah et",
        "output": "XÉ™zÉ™r dÉ™nizi dÃ¼nyanÄ±n É™n bÃ¶yÃ¼k gÃ¶lÃ¼dÃ¼r vÉ™ AzÉ™rbaycan iqtisadiyyatÄ± Ã¼Ã§Ã¼n Ã§ox vacibdir."
    },
    {
        "instruction": "AzÉ™rbaycan dilinin xÃ¼susiyyÉ™tlÉ™ri nÉ™lÉ™rdir",
        "output": "AzÉ™rbaycan dili TÃ¼rk dillÉ™r ailÉ™sindÉ™ndir. LatÄ±n É™lifbasÄ± ilÉ™ yazÄ±lÄ±r vÉ™ agglÃ¼tinativ quruluÅŸa malikdir."
    },
    {
        "instruction": "QarabaÄŸ haqqÄ±nda mÉ™lumat ver",
        "output": "QarabaÄŸ AzÉ™rbaycanÄ±n tarixi É™razisidir vÉ™ zÉ™ngin mÉ™dÉ™ni irsÉ™ malikdir."
    },
    {
        "instruction": "AzÉ™rbaycanÄ±n milli musiqi alÉ™tlÉ™rini say",
        "output": "AzÉ™rbaycanÄ±n milli musiqi alÉ™tlÉ™ri arasÄ±nda tar, kamanÃ§a, balaban, zurna, naÄŸara var."
    },
    {
        "instruction": "Ä°ngilis dilindÉ™n AzÉ™rbaycan dilinÉ™ tÉ™rcÃ¼mÉ™ et: Good morning",
        "output": "SabahÄ±nÄ±z xeyir"
    },
    {
        "instruction": "Ä°ngilis dilindÉ™n AzÉ™rbaycan dilinÉ™ tÉ™rcÃ¼mÉ™ et: Thank you",
        "output": "TÉ™ÅŸÉ™kkÃ¼r edirÉ™m"
    }
]

# Format data for training
formatted_data = []
for item in azerbaijani_data:
    text = f"### TÉ™limat:\n{item['instruction']}\n\n### Cavab:\n{item['output']}<|endoftext|>"
    formatted_data.append({"text": text})

print(f"âœ… Created {len(formatted_data)} training examples")
print("ğŸ“ Sample training text:")
print(formatted_data[0]["text"])

# %% [markdown]
# ## ğŸ¤– Step 3: Load Base Model and Tokenizer

# %%
# Load model and tokenizer
model_name = "microsoft/DialoGPT-small"  # Small model for quick training
print(f"ğŸ“¥ Loading model: {model_name}")

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto" if torch.cuda.is_available() else None
)

# Add padding token
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

print("âœ… Model and tokenizer loaded successfully!")
print(f"ğŸ§  Model parameters: {model.num_parameters():,}")

# %% [markdown]
# ## ğŸ”„ Step 4: Prepare Dataset for Training

# %%
# Tokenize the dataset
def tokenize_function(examples):
    return tokenizer(
        examples["text"],
        truncation=True,
        padding=False,
        max_length=512,
        return_tensors=None,
    )

# Create dataset
dataset = Dataset.from_list(formatted_data)
tokenized_dataset = dataset.map(
    tokenize_function,
    batched=True,
    remove_columns=dataset.column_names,
)

print(f"âœ… Dataset tokenized: {len(tokenized_dataset)} examples")

# %% [markdown]
# ## ğŸ‹ï¸ Step 5: Set Up Training Configuration

# %%
# Training arguments
training_args = TrainingArguments(
    output_dir="./azerbaijani_model",
    overwrite_output_dir=True,
    num_train_epochs=5,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    learning_rate=5e-5,
    weight_decay=0.01,
    warmup_steps=100,
    logging_steps=10,
    save_steps=100,
    save_total_limit=2,
    prediction_loss_only=True,
    remove_unused_columns=False,
)

# Data collator
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False,
)

print("âœ… Training configuration set up!")

# %% [markdown]
# ## ğŸš€ Step 6: Start Fine-tuning!

# %%
# Initialize trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    data_collator=data_collator,
    tokenizer=tokenizer,
)

print("ğŸš€ Starting fine-tuning...")
print("This will take a few minutes...")

# Start training
trainer.train()

print("âœ… Fine-tuning completed!")

# %% [markdown]
# ## ğŸ’¾ Step 7: Save the Fine-tuned Model

# %%
# Save model and tokenizer
output_dir = "./fine_tuned_azerbaijani_model"
trainer.save_model(output_dir)
tokenizer.save_pretrained(output_dir)

print(f"âœ… Model saved to: {output_dir}")

# %% [markdown]
# ## ğŸ§ª Step 8: Test Your Fine-tuned Model

# %%
# Load the fine-tuned model for testing
model.eval()

def generate_response(prompt, max_length=100):
    inputs = tokenizer.encode(prompt, return_tensors="pt")
    if torch.cuda.is_available():
        inputs = inputs.cuda()

    with torch.no_grad():
        outputs = model.generate(
            inputs,
            max_length=inputs.size(1) + max_length,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id,
            repetition_penalty=1.1
        )

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response[len(tokenizer.decode(inputs[0], skip_special_tokens=True)):].strip()

# Test questions
test_prompts = [
    "### TÉ™limat:\nAzÉ™rbaycanda neÃ§É™ iqlim qurÅŸaÄŸÄ± var?\n\n### Cavab:\n",
    "### TÉ™limat:\nBakÄ±nÄ±n tarixi haqqÄ±nda danÄ±ÅŸ\n\n### Cavab:\n",
    "### TÉ™limat:\nAzÉ™rbaycan mÉ™tbÉ™xindÉ™n nÉ™lÉ™r var?\n\n### Cavab:\n",
    "### TÉ™limat:\nNovruz bayramÄ± nÉ™dir?\n\n### Cavab:\n",
]

print("ğŸ§ª Testing your fine-tuned Azerbaijani model:")
print("=" * 50)

for i, prompt in enumerate(test_prompts, 1):
    response = generate_response(prompt)
    print(f"\nğŸ”¸ Test {i}:")
    print(f"â“ Prompt: {prompt.split('###')[1].strip()}")
    print(f"ğŸ¤– AI Response: {response}")

# %% [markdown]
# ## ğŸ¯ Step 9: Interactive Testing

# %%
# Interactive testing function
def ask_model(question):
    prompt = f"### TÉ™limat:\n{question}\n\n### Cavab:\n"
    response = generate_response(prompt, max_length=150)
    return response

# Test it yourself!
print("ğŸ¯ Your Azerbaijani AI is ready!")
print("Try asking questions like:")
print("- AzÉ™rbaycan haqqÄ±nda danÄ±ÅŸ")
print("- BakÄ±nÄ±n É™hÉ™miyyÉ™ti nÉ™dir?")
print("- AzÉ™rbaycan mÉ™tbÉ™xindÉ™n nÉ™lÉ™r var?")

# Example usage
question = "AzÉ™rbaycan haqqÄ±nda maraqlÄ± fakt sÃ¶ylÉ™"
response = ask_model(question)
print(f"\nğŸ‡¦ğŸ‡¿ Sual: {question}")
print(f"ğŸ¤– Cavab: {response}")

# %% [markdown]
# ## ğŸ“Š Step 10: Model Information and Download

# %%
# Display model information
print("ğŸ“Š YOUR FINE-TUNED AZERBAIJANI MODEL")
print("=" * 40)
print(f"âœ… Model successfully fine-tuned on {len(azerbaijani_data)} Azerbaijani examples")
print(f"ğŸ“ Model saved at: {output_dir}")
print(f"ğŸ§  Base model: {model_name}")
print(f"ğŸ”¥ Training device: {'GPU' if torch.cuda.is_available() else 'CPU'}")
print(f"ğŸ“ˆ Training epochs: {training_args.num_train_epochs}")
print(f"âš¡ Learning rate: {training_args.learning_rate}")

# Function to download model files
def download_model():
    """Download the fine-tuned model files"""
    import zipfile
    import shutil

    # Create zip file with model
    zip_path = "azerbaijani_model.zip"
    with zipfile.ZipFile(zip_path, 'w') as zipf:
        for root, dirs, files in os.walk(output_dir):
            for file in files:
                file_path = os.path.join(root, file)
                arcname = os.path.relpath(file_path, output_dir)
                zipf.write(file_path, arcname)

    print(f"ğŸ“¦ Model packaged as: {zip_path}")
    return zip_path

# Uncomment the next line to download your model
# download_model()

print("\nğŸ‰ CONGRATULATIONS!")
print("ğŸ‡¦ğŸ‡¿ You now have a fine-tuned Azerbaijani language model!")
print("ğŸš€ The model can understand and respond to Azerbaijani questions!")

# %% [markdown]
# ## ğŸ”§ Bonus: Save Model for Later Use

# %%
# Save a simple usage script
usage_code = '''
# Usage script for your fine-tuned Azerbaijani model
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# Load your fine-tuned model
model_path = "./fine_tuned_azerbaijani_model"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path)

def ask_azerbaijani_ai(question):
    prompt = f"### TÉ™limat:\\n{question}\\n\\n### Cavab:\\n"
    inputs = tokenizer.encode(prompt, return_tensors="pt")

    with torch.no_grad():
        outputs = model.generate(
            inputs,
            max_length=inputs.size(1) + 100,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response[len(prompt):].strip()

# Example usage
response = ask_azerbaijani_ai("AzÉ™rbaycan haqqÄ±nda danÄ±ÅŸ")
print(response)
'''

with open("use_model.py", "w", encoding="utf-8") as f:
    f.write(usage_code)

print("ğŸ“„ Usage script saved as 'use_model.py'")
print("ğŸ¯ You can use this script to load and use your model anywhere!")

# %%